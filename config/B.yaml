# M.yaml #Dipole Retain LSTM_based HiTANet ATTN Mamba MBTCN AdaCare ConCare TimesNet Informer PatchTST iTransformer TNAM Crossformer
train:
  dataset: B
  epochs: 30
  batch_size: 32
  learning_rate: 0.001
  seed: 42
  nfold: 5
  gpu: 1
  model: TNAM
  data_path: data/processed_data.csv
  model_path: models/
  log_path: logs/
  reload: True
  hours: None

models:
  TNAM:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  Mamba:
    seq_len: null
    d_model: 256
    hidden_size: 256
    state_size: 32
    batch_size: 32
    num_layers: 1
    dropout_rate: 0.2
  LSTM_based:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  HiTANet:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 2
  Retain:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  MGPTCN:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    kernel_size: 50
  MBTCN:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    kernel_size: 50
  Dipole:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  ATTN:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  AdaCare:
    input_dim: null
    hidden_size: 128
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  ConCare:
    input_dim: null
    hidden_size: 64
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  TimesNet:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  Informer:
    input_dim: null
    hidden_size: 64
    num_layers: 1
    dropout_rate: 0.5
    time: null
    heads: 4
  Autoformer:
    input_dim: null
    hidden_size: 16
    num_layers: 5
    dropout_rate: 0.1
    time: null
    heads: 4
  PatchTST:
    input_dim: null
    hidden_size: 16
    num_layers: 3
    dropout_rate: 0.1
    time: null
    heads: 2
    patch_len: 10
    stride: 4
  iTransformer:
    input_dim: null
    hidden_size: 16
    num_layers: 4
    dropout_rate: 0.1
    time: null
    heads: 1
  Crossformer:
    input_dim: null
    hidden_size: 32
    num_layers: 1
    dropout_rate: 0.7
    time: null
    heads: 2





#Dipole ATTN HiTANet LSTM_based Mamba MBTCN Retain TNAM AdaCare ConCare TimesNet Informer Autoformer PatchTST iTransformer