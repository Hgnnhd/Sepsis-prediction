# M.yaml
models:
  TNAM:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  TNAM_v:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  Mamba:
    seq_len: null
    d_model: 256
    hidden_size: 256
    state_size: 32
    batch_size: 32
    num_layers: 1
    dropout_rate: 0.2
  LSTM_based:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  HiTANet:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 2
  Retain:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  MBTCN:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    kernel_size: 50
  Dipole:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
  ATTN:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  AdaCare:
    input_dim: null
    hidden_size: 128
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  ConCare:
    input_dim: null
    hidden_size: 64
    num_layers: 1
    dropout_rate: 0.5
    time: null
    heads: 4
  TimesNet:
    input_dim: null
    hidden_size: 256
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
  Informer:
    input_dim: null
    hidden_size: 512
    num_layers: 1
    dropout_rate: 0.5
    time: null
    heads: 2
  Autoformer:
    input_dim: null
    hidden_size: 32
    num_layers: 1
    dropout_rate: 0.0
    time: null
    heads: 1
  PatchTST:
    input_dim: null
    hidden_size: 128
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 4
    patch_len: 2
    stride: 2
  iTransformer:
    input_dim: null
    hidden_size: 32
    num_layers: 1
    dropout_rate: 0.5
    time: null
    heads: 2
  Crossformer:
    input_dim: null
    hidden_size: 128
    num_layers: 1
    dropout_rate: 0.2
    time: null
    heads: 1

train:
  dataset: A
  epochs: 30
  batch_size: 32
  learning_rate: 0.001
  seed: 42
  nfold: 5
  gpu: 1
  model: TNAM
  data_path: data/processed_data.csv
  model_path: models/
  log_path: logs/
  reload: True
  hours: None


#Dipole Retain LSTM_based HiTANet LSTM_based Mamba MBTCN AdaCare ConCare TimesNet Informer PatchTST iTransformer TNAM Crossformer